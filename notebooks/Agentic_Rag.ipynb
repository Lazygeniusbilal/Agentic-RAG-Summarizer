{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98e6bab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ddc1641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lxml in c:\\users\\bilal ahmad\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (6.0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: C:\\Users\\Bilal Ahmad\\AppData\\Local\\Programs\\Python\\Python313\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d88226dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
      "<feed xmlns=\"http://www.w3.org/2005/Atom\">\n",
      "  <link href=\"http://arxiv.org/api/query?search_query%3Dall%3Amachine%20learning%26id_list%3D%26start%3D0%26max_results%3D1\" rel=\"self\" type=\"application/atom+xml\"/>\n",
      "  <title type=\"html\">ArXiv Query: search_query=all:machine learning&amp;id_list=&amp;start=0&amp;max_results=1</title>\n",
      "  <id>http://arxiv.org/api/REmrP0ijDn1S2ro+OMK7IoW1GgY</id>\n",
      "  <updated>2025-07-18T00:00:00-04:00</updated>\n",
      "  <opensearch:totalResults xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">420116</opensearch:totalResults>\n",
      "  <opensearch:startIndex xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">0</opensearch:startIndex>\n",
      "  <opensearch:itemsPerPage xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">1</opensearch:itemsPerPage>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1909.03550v1</id>\n",
      "    <updated>2019-09-08T21:49:42Z</updated>\n",
      "    <published>2019-09-08T21:49:42Z</published>\n",
      "    <title>Lecture Notes: Optimization for Machine Learning</title>\n",
      "    <summary>  Lecture notes on optimization for machine learning, derived from a course at\n",
      "Princeton University and tutorials given in MLSS, Buenos Aires, as well as\n",
      "Simons Foundation, Berkeley.\n",
      "</summary>\n",
      "    <author>\n",
      "      <name>Elad Hazan</name>\n",
      "    </author>\n",
      "    <link href=\"http://arxiv.org/abs/1909.03550v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1909.03550v1\" rel=\"related\" type=\"application/pdf\"/>\n",
      "    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "  </entry>\n",
      "</feed>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import requests\n",
    "import ssl\n",
    "\n",
    "# Create an SSL context that ignores certificate verification (for development only)\n",
    "context = ssl._create_unverified_context()\n",
    "\n",
    "# arXiv API URL for machine learning papers\n",
    "url = 'https://export.arxiv.org/api/query?search_query=all:machine+learning&start=0&max_results=1'\n",
    "\n",
    "# Send the request and read the response\n",
    "with urllib.request.urlopen(url, context=context) as response:\n",
    "    data = response.read().decode('utf-8')\n",
    "    print(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b924d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "url= 'http://arxiv.org/pdf/1909.03550v1'\n",
    "r= requests.get(url= url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22534420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Summary:\n",
      " Vision Transformers, such as LW-Transformer, are better than previous vision models due to their ability to reduce the number of parameters and computations while maintaining efficient attention modeling and feature transformation. Experimental results show that LW-Transformer achieves competitive performance for vision-and-language tasks. \n",
      "In a real-world example, this improved efficiency of LW-Transformer could be applied in image classification tasks, such as identifying objects in a large dataset of images. By reducing the computational cost while maintaining performance, LW-Transformer can help process and analyze images faster and more accurately in various applications like medical imaging or autonomous driving systems.\n",
      "PDF saved to: Summarized_Report.pdf\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import START, END, StateGraph\n",
    "from langchain_core.tools import tool\n",
    "from typing import TypedDict\n",
    "from pydantic import Field\n",
    "from pypdf import PdfReader\n",
    "from io import BytesIO\n",
    "import urllib.request\n",
    "import ssl\n",
    "from bs4 import BeautifulSoup\n",
    "from fpdf import FPDF\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# load the api key\n",
    "load_dotenv()\n",
    "\n",
    "# Define state structure\n",
    "class SummarizerState(TypedDict):\n",
    "    user_query: str\n",
    "    keyword: str\n",
    "    content: list[str]\n",
    "    summary: str\n",
    "    report: object\n",
    "\n",
    "# LangChain model\n",
    "llm = ChatOpenAI()  # Replace with your OpenAI API key\n",
    "\n",
    "# Keyword Extraction Node\n",
    "def keyword_extraction(state: SummarizerState):\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"\n",
    "        Extract only 1 keyword (max 2 words) from the query below:\n",
    "        {user_query}\n",
    "\n",
    "        Conditions:\n",
    "        - Combine multi-word keywords with a + sign like machine+learning\n",
    "        - Do not use more than 2 words\n",
    "        - Do not return more than 1 keyword\n",
    "\n",
    "        Example: For \"How can AI help in medical diagnosis?\", return \"medical+AI\"\n",
    "        \"\"\",\n",
    "        input_variables=[\"user_query\"]\n",
    "    )\n",
    "    result = llm.invoke(prompt.format(user_query=state['user_query']))\n",
    "    return {\"keyword\": result.content.strip()}\n",
    "\n",
    "# Retrieve PDF Text using arXiv API\n",
    "def retrieve_docs(state: SummarizerState):\n",
    "    try:\n",
    "        query = state[\"keyword\"].replace(\" \", \"+\")\n",
    "        url = f'https://export.arxiv.org/api/query?search_query=all:{query}&start=0&max_results=10'\n",
    "        context = ssl._create_unverified_context()\n",
    "        \n",
    "        with urllib.request.urlopen(url, context=context) as response:\n",
    "            data = response.read().decode('utf-8')\n",
    "        \n",
    "        soup = BeautifulSoup(data, 'xml')\n",
    "        pdf_url = soup.find('link', attrs={'title': 'pdf'}).get('href')\n",
    "        \n",
    "        with urllib.request.urlopen(pdf_url, context=context) as pdf_response:\n",
    "            pdf_data = pdf_response.read()\n",
    "        \n",
    "        doc = PdfReader(BytesIO(pdf_data))\n",
    "        content = []\n",
    "        for page in doc.pages:\n",
    "            text = page.extract_text()\n",
    "            if text:\n",
    "                content.append(text)\n",
    "        \n",
    "        return {\"content\": content}\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(\"Error retrieving document:\", e)\n",
    "        return {\"content\": [\"No document found.\"]}\n",
    "\n",
    "# Summarization Node\n",
    "def summarizer_agent(state: SummarizerState):\n",
    "    combined_text = \"\\n\".join(state[\"content\"])[:3000]  # Truncate if too long\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"\n",
    "        Given the following content, summarize it to answer the user's query: {user_query}\n",
    "\n",
    "        Then explain a real-world example of how the summarized knowledge can solve a practical problem.\n",
    "\n",
    "        Content:\n",
    "        {content}\n",
    "        \"\"\",\n",
    "        input_variables=[\"user_query\", \"content\"]\n",
    "    )\n",
    "    result = llm.invoke(prompt.format(user_query=state['user_query'], content=combined_text))\n",
    "    return {\"summary\": result.content.strip()}\n",
    "\n",
    "# PDF Report Generation Tool\n",
    "@tool\n",
    "def generate_pdf_tool(summary: str) -> str:\n",
    "    \"\"\"Generate a well-formatted PDF from the summary and return the file path.\"\"\"\n",
    "    pdf = FPDF()\n",
    "    pdf.add_page()\n",
    "    \n",
    "    # Title\n",
    "    pdf.set_font(\"Times\", \"B\", 16)\n",
    "    pdf.cell(0, 10, \"Research Summary Report\", ln=True, align=\"C\")\n",
    "    pdf.ln(10)\n",
    "\n",
    "    # Summary Section\n",
    "    pdf.set_font(\"Times\", \"B\", 14)\n",
    "    pdf.cell(0, 10, \"Summary\", ln=True)\n",
    "    pdf.set_font(\"Times\", \"\", 12)\n",
    "    pdf.multi_cell(0, 10, summary)\n",
    "\n",
    "    # Real-World Example Section\n",
    "    pdf.ln(8)\n",
    "    pdf.set_font(\"Times\", \"B\", 14)\n",
    "    pdf.cell(0, 10, \"Real-World Application\", ln=True)\n",
    "    pdf.set_font(\"Times\", \"\", 12)\n",
    "    pdf.multi_cell(0, 10,\n",
    "        \"Example: A hospital uses Vision Transformers to detect pneumonia from X-ray images. \"\n",
    "        \"Unlike older CNNs, Vision Transformers process the whole image contextually, identifying patterns even in blurry or complex regions. \"\n",
    "        \"This reduces missed cases and improves patient outcomes.\"\n",
    "    )\n",
    "\n",
    "    output_path = \"Summarized_Report.pdf\"\n",
    "    pdf.output(output_path)\n",
    "    return output_path\n",
    "\n",
    "# Final Node to Generate PDF\n",
    "def generate_report(state: SummarizerState):\n",
    "    report_path = generate_pdf_tool.invoke({\"summary\": state[\"summary\"]})\n",
    "    return {\"report\": report_path}\n",
    "\n",
    "# Create the graph\n",
    "graph = StateGraph(state_schema=SummarizerState)\n",
    "graph.add_node(\"keyword_extraction\", keyword_extraction)\n",
    "graph.add_node(\"retrieve_docs\", retrieve_docs)\n",
    "graph.add_node(\"summarizer_agent\", summarizer_agent)\n",
    "graph.add_node(\"generate_report\", generate_report)\n",
    "\n",
    "graph.add_edge(START, \"keyword_extraction\")\n",
    "graph.add_edge(\"keyword_extraction\", \"retrieve_docs\")\n",
    "graph.add_edge(\"retrieve_docs\", \"summarizer_agent\")\n",
    "graph.add_edge(\"summarizer_agent\", \"generate_report\")\n",
    "graph.add_edge(\"generate_report\", END)\n",
    "\n",
    "# Compile the graph\n",
    "app = graph.compile()\n",
    "\n",
    "# Run the flow\n",
    "final_output = app.invoke({\n",
    "    \"user_query\": \"How is Vision Transformers better than previous Vision models?\"\n",
    "})\n",
    "\n",
    "print(\"Final Summary:\\n\", final_output['summary'])\n",
    "print(\"PDF saved to:\", final_output['report'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db2370f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
